{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sahan-Daksh/rag-test/blob/main/SDGP_implementation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D6SlvJ-fCgP_",
        "outputId": "43a57967-a418-47d2-d115-5e71535f0ac0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting groq\n",
            "  Downloading groq-0.13.0-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from groq) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from groq) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from groq) (0.27.2)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from groq) (2.9.2)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from groq) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from groq) (4.12.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->groq) (3.10)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->groq) (1.2.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->groq) (2024.8.30)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->groq) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->groq) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->groq) (2.23.4)\n",
            "Downloading groq-0.13.0-py3-none-any.whl (108 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/108.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m102.4/108.8 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m108.8/108.8 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: groq\n",
            "Successfully installed groq-0.13.0\n"
          ]
        }
      ],
      "source": [
        "!pip install groq\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "installed Grok dependecy sample code for pre defined query"
      ],
      "metadata": {
        "id": "brQgnDEyJCup"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from groq import Groq\n",
        "\n",
        "client = Groq(\n",
        "\n",
        "        api_key=\"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\"\n",
        ")\n",
        "\n",
        "chat_completion = client.chat.completions.create(\n",
        "    #\n",
        "    # Required parameters\n",
        "    #\n",
        "    messages=[\n",
        "        # Set an optional system message. This sets the behavior of the\n",
        "        # assistant and can be used to provide specific instructions for\n",
        "        # how it should behave throughout the conversation.\n",
        "        {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": \"you are a helpful assistant.\"\n",
        "        },\n",
        "        # Set a user message for the assistant to respond to.\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": \"Explain the importance of fast language models\",\n",
        "        }\n",
        "    ],\n",
        "\n",
        "    # The language model which will generate the completion.\n",
        "    model=\"llama3-8b-8192\",\n",
        "\n",
        "    #\n",
        "    # Optional parameters\n",
        "    #\n",
        "\n",
        "    # Controls randomness: lowering results in less random completions.\n",
        "    # As the temperature approaches zero, the model will become deterministic\n",
        "    # and repetitive.\n",
        "    temperature=0.5,\n",
        "\n",
        "    # The maximum number of tokens to generate. Requests can use up to\n",
        "    # 32,768 tokens shared between prompt and completion.\n",
        "    max_tokens=1024,\n",
        "\n",
        "    # Controls diversity via nucleus sampling: 0.5 means half of all\n",
        "    # likelihood-weighted options are considered.\n",
        "    top_p=1,\n",
        "\n",
        "    # A stop sequence is a predefined or user-specified text string that\n",
        "    # signals an AI to stop generating content, ensuring its responses\n",
        "    # remain focused and concise. Examples include punctuation marks and\n",
        "    # markers like \"[end]\".\n",
        "    stop=None,\n",
        "\n",
        "    # If set, partial message deltas will be sent.\n",
        "    stream=False,\n",
        ")\n",
        "\n",
        "# Print the completion returned by the LLM.\n",
        "print(chat_completion.choices[0].message.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xHAoovtvCrp5",
        "outputId": "a0c7410d-4e66-4d8b-bf1d-aaa12dd4210d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fast language models have gained significant attention in recent years due to their ability to process and generate human-like language quickly and efficiently. Here are some key reasons why fast language models are important:\n",
            "\n",
            "1. **Real-time applications**: Fast language models enable real-time language processing, which is crucial for applications such as chatbots, virtual assistants, and language translation systems. They can quickly respond to user queries, making them more interactive and user-friendly.\n",
            "2. **Scalability**: Fast language models can handle large volumes of text data, making them ideal for big data applications. They can process and analyze vast amounts of data quickly, allowing for faster insights and decision-making.\n",
            "3. **Improved user experience**: Fast language models can provide instant responses to user queries, reducing wait times and improving the overall user experience. This is particularly important for applications with high volumes of traffic, such as search engines or customer support systems.\n",
            "4. **Enhanced language understanding**: Fast language models can analyze and understand language patterns and nuances more accurately, enabling more accurate language translation, sentiment analysis, and text summarization.\n",
            "5. **Faster innovation**: Fast language models accelerate the development of new language-related applications and services. They enable researchers and developers to test and refine their ideas quickly, driving innovation and progress in the field.\n",
            "6. **Cost-effective**: Fast language models can reduce the cost of language-related tasks by automating processes that were previously time-consuming and labor-intensive. This can lead to significant cost savings for organizations and individuals.\n",
            "7. **Improved accessibility**: Fast language models can help bridge the language gap by providing instant translation and interpretation services. This can improve communication and collaboration between people who speak different languages.\n",
            "8. **Enhanced decision-making**: Fast language models can analyze large amounts of text data quickly, providing valuable insights and recommendations for businesses, governments, and individuals. This can inform decision-making and improve strategic planning.\n",
            "9. **Better customer service**: Fast language models can enable companies to provide 24/7 customer support, responding to customer inquiries and concerns in real-time. This can improve customer satisfaction and loyalty.\n",
            "10. **Advancements in AI**: Fast language models are a key component of artificial intelligence (AI) systems, enabling them to learn and improve over time. As AI continues to evolve, fast language models will play a crucial role in driving innovation and progress.\n",
            "\n",
            "In summary, fast language models are important because they enable real-time language processing, scalability, improved user experience, enhanced language understanding, faster innovation, cost-effectiveness, improved accessibility, enhanced decision-making, better customer service, and advancements in AI.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "code this a query to check the groq APi service"
      ],
      "metadata": {
        "id": "8FeRUXTGJRrB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from groq import Groq\n",
        "\n",
        "# Initialize the Groq client with the API key\n",
        "client = Groq(\n",
        "    api_key=\"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\"\n",
        ")\n",
        "\n",
        "# Prompt the user to input a query\n",
        "user_query = input(\"Enter your query: \")\n",
        "\n",
        "# Generate a chat completion based on the user query\n",
        "chat_completion = client.chat.completions.create(\n",
        "    #\n",
        "    # Required parameters\n",
        "    #\n",
        "    messages=[\n",
        "        # Set an optional system message. This sets the behavior of the\n",
        "        # assistant and can be used to provide specific instructions for\n",
        "        # how it should behave throughout the conversation.\n",
        "        {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": \"You are a helpful assistant.\"\n",
        "        },\n",
        "        # Set a user message for the assistant to respond to.\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": user_query,\n",
        "        }\n",
        "    ],\n",
        "\n",
        "    # The language model which will generate the completion.\n",
        "    model=\"llama3-8b-8192\",\n",
        "\n",
        "    #\n",
        "    # Optional parameters\n",
        "    #\n",
        "\n",
        "    # Controls randomness: lowering results in less random completions.\n",
        "    # As the temperature approaches zero, the model will become deterministic\n",
        "    # and repetitive.\n",
        "    temperature=0.5,\n",
        "\n",
        "    # The maximum number of tokens to generate. Requests can use up to\n",
        "    # 32,768 tokens shared between prompt and completion.\n",
        "    max_tokens=1024,\n",
        "\n",
        "    # Controls diversity via nucleus sampling: 0.5 means half of all\n",
        "    # likelihood-weighted options are considered.\n",
        "    top_p=1,\n",
        "\n",
        "    # A stop sequence is a predefined or user-specified text string that\n",
        "    # signals an AI to stop generating content, ensuring its responses\n",
        "    # remain focused and concise. Examples include punctuation marks and\n",
        "    # markers like \"[end]\".\n",
        "    stop=None,\n",
        "\n",
        "    # If set, partial message deltas will be sent.\n",
        "    stream=False,\n",
        ")\n",
        "\n",
        "# Print the completion returned by the LLM.\n",
        "print(\"Response from the model:\")\n",
        "print(chat_completion.choices[0].message.content)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        },
        "id": "Wnk7DVpaElIa",
        "outputId": "4ecee80f-c5b5-4df1-94e9-a2e8a7b562de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'groq'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-11847f23e57e>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgroq\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGroq\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Initialize the Groq client with the API key\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m client = Groq(\n\u001b[1;32m      5\u001b[0m     \u001b[0mapi_key\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"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\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'groq'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QHNY_zs3KWoJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from groq import Groq\n",
        "\n",
        "# Initialize the Groq client with the API key\n",
        "client = Groq(\n",
        "    api_key=\"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\"\n",
        ")\n",
        "\n",
        "# Prompt the user to input a query\n",
        "user_query = input(\"Enter your query: \")\n",
        "\n",
        "# Generate a chat completion based on the user query\n",
        "chat_completion = client.chat.completions.create(\n",
        "    #\n",
        "    # Required parameters\n",
        "    #\n",
        "    messages=[\n",
        "        # Set an optional system message. This sets the behavior of the\n",
        "        # assistant and can be used to provide specific instructions for\n",
        "        # how it should behave throughout the conversation.\n",
        "        {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": \"You are a helpful assistant.\"\n",
        "        },\n",
        "        # Set a user message for the assistant to respond to.\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": user_query,\n",
        "        }\n",
        "    ],\n",
        "\n",
        "    # The language model which will generate the completion.\n",
        "    model=\"llama3-8b-8192\",\n",
        "\n",
        "    #\n",
        "    # Optional parameters\n",
        "    #\n",
        "\n",
        "    # Controls randomness: lowering results in less random completions.\n",
        "    # As the temperature approaches zero, the model will become deterministic\n",
        "    # and repetitive.\n",
        "    temperature=0.5,\n",
        "\n",
        "    # The maximum number of tokens to generate. Requests can use up to\n",
        "    # 32,768 tokens shared between prompt and completion.\n",
        "    max_tokens=1024,\n",
        "\n",
        "    # Controls diversity via nucleus sampling: 0.5 means half of all\n",
        "    # likelihood-weighted options are considered.\n",
        "    top_p=1,\n",
        "\n",
        "    # A stop sequence is a predefined or user-specified text string that\n",
        "    # signals an AI to stop generating content, ensuring its responses\n",
        "    # remain focused and concise. Examples include punctuation marks and\n",
        "    # markers like \"[end]\".\n",
        "    stop=None,\n",
        "\n",
        "    # If set, partial message deltas will be sent.\n",
        "    stream=False,\n",
        ")\n",
        "\n",
        "# Print the completion returned by the LLM.\n",
        "print(\"Response from the model:\")\n",
        "print(chat_completion.choices[0].message.content)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ad11cc6f-b278-4184-d18c-4eedee04aeec",
        "id": "mvB0qC1pKXTw"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter your query: tell me the ideas about bmw i8\n",
            "Response from the model:\n",
            "The BMW i8! Here are some exciting ideas and features about this revolutionary plug-in hybrid supercar:\n",
            "\n",
            "1. **Design**: The i8's sleek and futuristic design is a marvel. Its gull-wing doors, curved lines, and LED lights create a striking visual presence. The carbon fiber-reinforced passenger cell and aluminum chassis provide exceptional strength and lightness.\n",
            "2. **Plug-in Hybrid Powertrain**: The i8 combines a 1.5-liter, 3-cylinder turbocharged gasoline engine with an electric motor. The gasoline engine produces 231 horsepower, while the electric motor adds 131 horsepower. The total output is 362 horsepower, with a combined torque of 420 lb-ft.\n",
            "3. **Electric-only Mode**: The i8 can run solely on electric power for up to 18 miles (29 km) and 62 mph (100 km/h), making it an eco-friendly option for short trips or urban driving.\n",
            "4. **Sport Mode**: Switch to Sport mode, and the i8 transforms into a high-performance machine. The electric motor and gasoline engine work in tandem to deliver explosive acceleration, with 0-60 mph (0-97 km/h) taking just 4.2 seconds.\n",
            "5. **Carbon Fiber Chassis**: The i8's carbon fiber chassis provides exceptional strength, lightness, and precision handling. It also helps reduce weight and improve fuel efficiency.\n",
            "6. **Advanced Safety Features**: The i8 comes equipped with advanced safety features like lane departure warning, forward collision warning, and automatic emergency braking.\n",
            "7. **Luxurious Interior**: The i8's interior is designed with luxury and comfort in mind. The seats are upholstered in premium leather, and the dashboard features a high-resolution display screen.\n",
            "8. **Regenerative Braking**: The i8's regenerative braking system captures kinetic energy and converts it into electrical energy, which is stored in the high-voltage battery.\n",
            "9. **Efficient Aerodynamics**: The i8's aerodynamic design helps reduce air resistance, improving fuel efficiency and reducing wind noise.\n",
            "10. **Customization Options**: The i8 offers various customization options, including different wheel designs, interior trim, and exterior colors, allowing owners to personalize their vehicle to their taste.\n",
            "11. **Sustainable Production**: The i8 is produced using sustainable materials and processes, reducing its environmental impact.\n",
            "12. **Technology Integration**: The i8 features BMW's iDrive infotainment system, which integrates with the vehicle's advanced safety features and provides seamless connectivity.\n",
            "\n",
            "These innovative features and ideas make the BMW i8 a truly unique and exciting vehicle, offering a blend of performance, efficiency, and sustainability.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answering based on document.extracted text from pdf stored in to a variable and it passes to the classs"
      ],
      "metadata": {
        "id": "Taw8o_jVJb1T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install PyPDF2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u4fpSXjwK9aI",
        "outputId": "63b064d2-7bb7-4de7-85c3-e8b06faf52fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting PyPDF2\n",
            "  Downloading pypdf2-3.0.1-py3-none-any.whl.metadata (6.8 kB)\n",
            "Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PyPDF2\n",
            "Successfully installed PyPDF2-3.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install PyPDF2 faiss-cpu sentence-transformers openai\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PZwqVsBSSLpj",
        "outputId": "7c4aac49-a521-45cf-82a4-3e9a15a2c22b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: PyPDF2 in /usr/local/lib/python3.10/dist-packages (3.0.1)\n",
            "Collecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.9.0.post1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.4 kB)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.10/dist-packages (3.2.1)\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.10/dist-packages (1.54.4)\n",
            "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.10/dist-packages (from faiss-cpu) (1.26.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from faiss-cpu) (24.2)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.46.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.66.6)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (2.5.1+cu121)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.5.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (0.26.2)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (11.0.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.27.2)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.7.1)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.9.2)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.10/dist-packages (from openai) (4.12.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.8.30)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.16.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2024.10.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.23.4)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.9.11)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.20.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.2.3)\n",
            "Downloading faiss_cpu-1.9.0.post1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (27.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.5/27.5 MB\u001b[0m \u001b[31m31.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: faiss-cpu\n",
            "Successfully installed faiss-cpu-1.9.0.post1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import PyPDF2\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import faiss\n",
        "import numpy as np\n",
        "import openai\n",
        "from groq import Groq\n",
        "\n",
        "# OpenAI API Key (Replace with your API key)\n",
        "openai.api_key = \"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\"\n",
        "\n",
        "# Initialize Groq client\n",
        "client = Groq(api_key=\"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\")\n",
        "\n",
        "# Step 1: Extract text from PDF\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    text = \"\"\n",
        "    with open(pdf_path, \"rb\") as file:\n",
        "        reader = PyPDF2.PdfReader(file)\n",
        "        for page in reader.pages:\n",
        "            text += page.extract_text()\n",
        "    return text\n",
        "\n",
        "# Step 2: Split text into chunks\n",
        "def split_text_into_chunks(text, chunk_size=300):\n",
        "    words = text.split()\n",
        "    chunks = [\" \".join(words[i:i + chunk_size]) for i in range(0, len(words), chunk_size)]\n",
        "    return chunks\n",
        "\n",
        "# Step 3: Generate embeddings for chunks\n",
        "def generate_embeddings(text_chunks, model_name=\"all-MiniLM-L6-v2\"):\n",
        "    model = SentenceTransformer(model_name)\n",
        "    embeddings = model.encode(text_chunks)\n",
        "    return embeddings\n",
        "\n",
        "# Step 4: Store embeddings in FAISS\n",
        "def create_faiss_index(embeddings):\n",
        "    dimension = embeddings.shape[1]\n",
        "    index = faiss.IndexFlatL2(dimension)  # L2 similarity\n",
        "    index.add(embeddings)\n",
        "    return index\n",
        "\n",
        "# Step 5: Retrieve relevant chunks\n",
        "def retrieve_relevant_chunks(query, text_chunks, index, model):\n",
        "    query_embedding = model.encode([query])\n",
        "    distances, indices = index.search(query_embedding, k=3)  # Retrieve top 3 matches\n",
        "    return [text_chunks[idx] for idx in indices[0]]\n",
        "\n",
        "# Step 6: Query the language model\n",
        "def query_llm(client, query, context):\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": \"You are a helpful assistant answering questions based on provided context.\"},\n",
        "        {\"role\": \"assistant\", \"content\": context},\n",
        "        {\"role\": \"user\", \"content\": query},\n",
        "    ]\n",
        "    response = client.chat.completions.create(\n",
        "        messages=messages,\n",
        "        model=\"llama3-8b-8192\",\n",
        "        temperature=0.5,\n",
        "        max_tokens=512,\n",
        "        top_p=1,\n",
        "        stop=None,\n",
        "        stream=False,\n",
        "    )\n",
        "    return response.choices[0].message.content\n",
        "\n",
        "# Main Code\n",
        "pdf_path = \"/content/drive/MyDrive/Bash.pdf\"\n",
        "\n",
        "# Step 1: Extract text\n",
        "document_text = extract_text_from_pdf(pdf_path)\n",
        "\n",
        "# Step 2: Split into chunks\n",
        "text_chunks = split_text_into_chunks(document_text)\n",
        "\n",
        "# Step 3: Generate embeddings\n",
        "embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "embeddings = generate_embeddings(text_chunks, model_name=\"all-MiniLM-L6-v2\")\n",
        "\n",
        "# Step 4: Create FAISS index\n",
        "faiss_index = create_faiss_index(np.array(embeddings))\n",
        "\n",
        "# Step 5: User Query\n",
        "user_query = input(\"Enter your query: \")\n",
        "\n",
        "# Step 6: Retrieve relevant chunks\n",
        "relevant_chunks = retrieve_relevant_chunks(user_query, text_chunks, faiss_index, embedding_model)\n",
        "\n",
        "# Combine chunks into context\n",
        "context = \" \".join(relevant_chunks)\n",
        "\n",
        "# Step 7: Query the LLM\n",
        "response = query_llm(client, user_query, context)\n",
        "\n",
        "print(\"\\nResponse from the model:\")\n",
        "print(response)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0JeXN42_SenP",
        "outputId": "87cd3561-0ea4-4db8-b1e6-2babc14974bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter your query: how to create a bash script file\n",
            "\n",
            "Response from the model:\n",
            "To create a bash script file, you can follow these steps:\n",
            "\n",
            "1. Open a text editor: You can use any text editor like nano, vim, or gedit to create a new file.\n",
            "2. Create a new file: Give the file a name with a `.sh` extension, for example, `my_script.sh`. This will indicate that it's a bash script file.\n",
            "3. Write the script: Start writing your bash script in the file. You can use the `#!/bin/bash` shebang at the beginning of the file to specify the interpreter that should be used to run the script.\n",
            "4. Save the file: Save the file with the `.sh` extension.\n",
            "5. Make the file executable: To run the script, you need to make it executable. You can do this by running the command `chmod +x my_script.sh` in the terminal.\n",
            "6. Run the script: Once the file is executable, you can run the script by typing `./my_script.sh` in the terminal.\n",
            "\n",
            "Here's an example of a simple bash script that prints \"Hello, World!\" to the screen:\n",
            "```bash\n",
            "#!/bin/bash\n",
            "\n",
            "echo \"Hello, World!\"\n",
            "```\n",
            "To create a bash script file using nano, you can follow these steps:\n",
            "\n",
            "1. Open nano: Type `nano` in the terminal to open the nano editor.\n",
            "2. Create a new file: Type `nano my_script.sh` to create a new file called `my_script.sh`.\n",
            "3. Write the script: Start writing your bash script in the file.\n",
            "4. Save the file: Press `Ctrl+X` to exit the editor, then press `Y` to save the file.\n",
            "5. Make the file executable: Type `chmod +x my_script.sh` in the terminal to make the file executable.\n",
            "6. Run the script: Type `./my_script.sh` in the terminal to run the script.\n",
            "\n",
            "Alternatively, you can also use other text editors like vim or gedit to create a bash script file.\n"
          ]
        }
      ]
    }
  ]
}